---
title: "COTS SDM_2018"
author: "Samuel Matthews"
date: "2 March 2018"
output: html_document
---

### Data Entry and Manipulation

* The first step is to load up Camille's selected environmental variables

```{r, echo=FALSE, message=FALSE, cache=F}
rm(list=ls())
source("Scripts/CoTSModel_UtilityFunctions.R")
loadPackages()
```

I still need to generate a weighted in-degree

```{r, echo=FALSE, cache=F}
XYZdata <- read.csv("Data/Environmental_data.csv", header=T)
# XYZdata = XYZdata %>% arrange(-y,x)
# XYZdata$PIXEL_ID = 1:16035
colnames(XYZdata)[2:3] = c("x", "y")
shp = rgdal::readOGR("Data/Indicative_Reef_boundary/Indicative_Reef_boundary.shp")
zone = rgdal::readOGR("Data/Great_Barrier_Reef_Marine_Park_Zoning/Great_Barrier_Reef_Marine_Park_Zoning.shp")
shp2 = rgdal::readOGR("Data/SDE_OWNER_crcgis_land250/SDE_OWNER_crcgis_land250.shp")
cotsconn = read.csv("../Chapter4_CoTS Connectivity/cotsconn_nodes.csv", header = T)
colnames(cotsconn)[2] = "REEF_NAME"
# add reefname to XYZ data

shpdata = shp@data %>% dplyr::select(LOC_NAME_L, X_COORD, Y_COORD) 
colnames(shpdata)[1] = "ReefName"
cotsconnmat = R.matlab::readMat("../Chapter4_CoTS Connectivity/cotsconn.mat")$cotsconn

reefs = read.csv("../Chapter4_CoTS Connectivity/reefs.csv", header = T) %>% pull(1)
row.names(cotsconnmat) = reefs

# Add eReefs Chlorophyll
Chl = read.csv("Data/eReefs_Chl_summary.csv", header=T)
Coral <- read.table("Data/CoralCovars.txt", header = T, sep="\t")
MRT = read.csv("Data/XYZ_bent_clust.csv", header=T)
colnames(MRT)[2:3] = c("x", "y")
MRT = MRT %>% arrange(-y,x)
MRT$PIXEL_ID = 1:length(MRT$x)
Chl = Chl %>% arrange(-y,x)
Chl$PIXEL_ID = 1:length(Chl$x)
colnames(Coral)[2:3] <- c("x", "y")
Coral = Coral %>% arrange(-y,x)
Coral$PIXEL_ID = 1:length(Coral$x)

XYZdata <- inner_join(XYZdata, Coral[-c(1,4)], by=c("x", "y"))
XYZdata <- inner_join(XYZdata, MRT[c(2:3,5:6)], by=c("x", "y"))
XYZdata <- inner_join(XYZdata, Chl[-c(1,4)], by=c("x", "y"))

coords = XYZdata[2:3]
coordinates(coords) = ~x+y
crs(coords) = crs(shp)
XYZdata$Zone = sp::over(coords,zone) %>% pull(TYPE)
XYZdata = inner_join(XYZdata, cotsconn[-1], by="REEF_NAME")
XYZdata = XYZdata[c(1:7,45:46,50, 8:44,47:49,51:61)]
```


### Disturbance Data

Summarise disturbance pressure up to 2012

```{r}
bleach = read.csv("Data/Bleaching_data_98_02_16.csv", header=T)
dhw = read.csv("Data/DHW_data.csv", header =T)
COTS = read.csv("Data/CoTS_data.csv", header=T)
cycl = read.csv("Data/Cyclones_data.csv", header=T) 
cycl.reef = cycl %>% gather(key=year, value=exposure, -c(PIXEL_ID:REEF_NAME)) %>% mutate(year=as.numeric(substr(year, 7,10))) %>%
  filter(year <= 2012) %>% group_by(REEF_NAME) %>% summarise(`CycExp85-12` = mean(exposure, na.rm = T))
cycl.reef = cycl %>% gather(key=year, value=exposure, -c(PIXEL_ID:REEF_NAME)) %>% mutate(year=as.numeric(substr(year, 7,10))) %>%
  filter(year <= 2012 & year >=2008) %>% group_by(REEF_NAME) %>% summarise(`CycExp08-12` = mean(exposure, na.rm = T)) %>% inner_join(cycl.reef)
bleach.reef = bleach %>% gather(key=year, value=exposure, -c(PIXEL_ID:REEF_NAME)) %>% mutate(year=as.numeric(substr(year, 8,11))) %>%
  filter(year <= 2012) %>% group_by(REEF_NAME) %>% summarise(`BleachExp85-12` = mean(exposure, na.rm = T))
dhw.reef = dhw %>% gather(key=year, value=exposure, -c(PIXEL_ID:REEF_NAME)) %>% mutate(year=as.numeric(substr(year, 11,14))) %>%
  filter(year <= 2012) %>% group_by(REEF_NAME) %>% summarise(`DHWExp85-12` = mean(exposure, na.rm = T))
dhw.reef = dhw %>% gather(key=year, value=exposure, -c(PIXEL_ID:REEF_NAME)) %>% mutate(year=as.numeric(substr(year, 11,14))) %>%
  filter(year <= 2012 & year >=2008) %>% group_by(REEF_NAME) %>% summarise(`DHWExp08-12` = mean(exposure, na.rm = T)) %>% inner_join(dhw.reef)
cots.reef = COTS %>% gather(key=year, value=exposure, -c(PIXEL_ID:REEF_NAME)) %>% mutate(year=as.numeric(substr(year, 6,9))) %>%
  filter(year <= 2012) %>% group_by(REEF_NAME) %>% summarise(`COTSExp85-12` = mean(exposure, na.rm = T))
cots.reef = COTS %>% gather(key=year, value=exposure, -c(PIXEL_ID:REEF_NAME)) %>% mutate(year=as.numeric(substr(year, 6,9))) %>%
  filter(year <= 2012 & year >=2008) %>% group_by(REEF_NAME) %>% summarise(`COTSExp08-12` = mean(exposure, na.rm = T)) %>% inner_join(cots.reef)
disturbance = inner_join(cycl.reef,bleach.reef) %>% inner_join(dhw.reef) %>% inner_join(cots.reef)

```

### CoTS Data

```{r}
eotr = read.csv("Data/database_report.csv", header = T) %>% dplyr::select(Observation.Date, Reef.ID,Reef.Name, Latitude, Longitude, Marine.Park.Zone, COTS.Seen..Adult.)
colnames(eotr) = c("Date", "ReefID", "ReefName", "y", "x", "Zone", "COTS")
eotr$COTS_PA = ifelse(eotr$COTS > 0, 1, eotr$COTS)
#manta[is.na(manta)] <- 0
COTS_PA = eotr %>% 
  group_by(ReefName, ReefID) %>%
  summarise(meanCOTS = mean(COTS),
            Surveys = length(COTS),
            COTS_PROP = as.integer(100*(sum(COTS_PA)/Surveys)), #ADD A PROPORTION OF SURVEYS VARIABLE
            COTS_PA = ifelse(sum(COTS>0),1,0)) %>%
  mutate(OubtreakStatus = ifelse(meanCOTS <=0.1, "No Outbreak",
                          ifelse(meanCOTS <=0.22, "Potential",
                          ifelse(meanCOTS <=1, "Established", "Severe")))) %>%
  filter(Surveys > 10)
colnames(COTS_PA)[1] = "REEF_NAME"
eotrSum = eotr %>% 
  group_by(ReefName, ReefID) %>%
  summarise(Surveys = length(COTS)) %>%
  inner_join(shpdata, by="ReefName")
# Create a column to look at abundance

eotr$qtr = zoo::as.yearqtr(as.Date(eotr$Date, "%d/%m/%Y"))
COTS_Out = eotr %>% 
  group_by(ReefName, ReefID, qtr) %>%
  summarise(meanCOTS = mean(COTS,na.omit=T),
            Surveys = length(COTS),
            sdCOTS = sd(COTS),
            COTS_Out = ifelse(meanCOTS>0.24,1,0)) %>%
  filter(Surveys >= 5) %>%
  summarise(COTS_OUT = ifelse(sum(COTS_Out > 0),1,0))
colnames(COTS_Out)[1] = "REEF_NAME"

```

### Heatmap of Survey Locations

```{r}
# Create heatmap of survey locations

plot(shp2, col="lightgrey", border="darkgrey", xlim=c(140,155), ylim=c(-25,-12), bg="white")
points(coords, pch=20, cex=0.6)

gbr <- get_map(location = c(lon=146, lat=-19), zoom =6, source = "google",maptype = "hybrid", color ="bw")
ggmap(gbr, extent = "device") +
  # geom_polygon(data = broom::tidy(shp),
  #                 aes(long, lat, group = group),
  #                 fill = "orange", colour = "white", alpha = 0.2) +
  # geom_density2d(data = eotrSum, aes(x = X_COORD, y = Y_COORD), size = 0.1) +
  stat_density2d(data = eotr,
                 aes(x = x, y = y, fill = ..level.., alpha = ..level..), size = 1,
                 bins = 16, geom = "polygon") + scale_fill_gradient(low="green", high="red") #+

```

### Summarise EnviroData To Reef Level

```{r}
XYZdata[, 11:61] <- sapply(XYZdata[, 11:61], as.numeric)
variables = colnames(XYZdata)[11:61]
XYZsum = XYZdata %>% 
  group_by(REEF_NAME) %>%
  summarise_at(variables, mean, na.rm=T)
cat.factors = XYZdata %>% 
  group_by(REEF_NAME) %>%
  count(bent.clust,bent.clust.18, Zone, SECTOR, CROSS_SHELF) %>% top_n(1) %>% dplyr::select(REEF_NAME, bent.clust, bent.clust.18, Zone,SECTOR, CROSS_SHELF)
XYZsum = inner_join(XYZsum,cat.factors, by="REEF_NAME") %>% inner_join(disturbance,by="REEF_NAME")


# COTS_PA = ungroup(COTS_PA) %>% inner_join(XYZsum, by="REEF_NAME")
# COTS_PA$COTS_PA <- as.integer(COTS_PA$COTS_PA)
# 
# COTS_Out = ungroup(COTS_Out) %>% inner_join(XYZsum, by="REEF_NAME") %>% na.omit()
# COTS_Out$COTS_OUT <- as.integer(COTS_Out$COTS_OUT)

# COTS_PA$Zone = factor(ifelse(COTS_PA$Zone %in% 
#                                 c("Habitat Protection", "Conservation Park", "Commonwealth Island (Other)"), "Open", "Closed"))
# COTS_Out$Zone = factor(ifelse(COTS_Out$Zone %in% 
#                                 c("Habitat Protection", "Conservation Park", "Commonwealth Island (Other)"), "Open", "Closed"))

# COTS_PA = COTS_PA[-which(duplicated(COTS_PA$ReefName)),]
```

### Add ACutocovariates

### Need to add autocovariates to the entire dataset and then scale

```{r}
# library(dplyr)
# library(gstat)
# colnames(shpdata)[1] = "REEF_NAME"
# COTS_PA = COTS_PA %>% inner_join(shpdata, by="REEF_NAME")
# sp:::coordinates(COTS_PA) = ~X_COORD+Y_COORD
# 
# matchedreefs = match(COTS_PA$REEF_NAME, reefs)
# cotsconn.matched = cotsconnmat[matchedreefs,matchedreefs]
# dim(cotsconn.matched)
# cotsconn.matched[1:10,1:10]
# # Distance matrix
# d <- rgeos::gDistance(COTS_PA, byid = T)
# AC = vector()
# COTS_PA = as.data.frame(COTS_PA)
# 
# ACConn = vector()
# 
# for(i in 1:length(COTS_PA$COTS_PA)) {
#   wj = 1/d[i,]
#   yj = COTS_PA$COTS_PA
#   wjyj = wj*yj
#   AC[i] = sum(wjyj[-i])
#   wj2 = cotsconn.matched[i,]
#   yj2 = COTS_PA$COTS_PA
#   wjyj2 =wj2*yj2
#   ACConn[i] = sum(wjyj2[-i])
# }
# COTS_PA$AC = as.vector(AC)
# COTS_PA$ACConn = as.vector(ACConn)
# COTS_PA = COTS_PA[c(1:7, 59:63,8:58, 64:74)]
# COTS_PA.sc = COTS_PA
# COTS_PA.sc[,13:74] = scale(COTS_PA[,13:74])
# COTS_PA.sc[8:9] = lapply(COTS_PA.sc[8:9], base::as.factor)
# COTS_PA[8:9] = lapply(COTS_PA[8:9], base::as.factor)
# 
# 
# # Same for COTS_OUT
# COTS_Out = COTS_Out %>% inner_join(shpdata, by="REEF_NAME")
# sp:::coordinates(COTS_Out) = ~X_COORD+Y_COORD
# 
# matchedreefs = match(COTS_Out$REEF_NAME, reefs)
# cotsconn.matched = cotsconnmat[matchedreefs,matchedreefs]
# dim(cotsconn.matched)
# cotsconn.matched[1:10,1:10]
# # Distance matrix
# d <- rgeos::gDistance(COTS_Out, byid = T)
# AC = vector()
# COTS_Out = as.data.frame(COTS_Out)
# 
# ACConn = vector()
# 
# for(i in 1:length(COTS_Out$COTS_OUT)) {
#   wj = 1/d[i,]
#   yj = COTS_Out$COTS_OUT
#   wjyj = wj*yj
#   AC[i] = sum(wjyj[-i])
#   wj2 = cotsconn.matched[i,]
#   yj2 = COTS_Out$COTS_OUT
#   wjyj2 =wj2*yj2
#   ACConn[i] = sum(wjyj2[-i])
# }



## To entire dataset
colnames(shpdata)[1] = "REEF_NAME"
XYZsum = XYZsum %>% inner_join(shpdata, by="REEF_NAME")
XYZsum.Conn = XYZsum
sp:::coordinates(XYZsum.Conn) = ~X_COORD+Y_COORD

matchedreefs = match(XYZsum.Conn$REEF_NAME, reefs)
cotsconn.matched = cotsconnmat[matchedreefs,matchedreefs]
dim(cotsconn.matched)
cotsconn.matched[1:10,1:10]
# Distance matrix
d <- rgeos::gDistance(XYZsum.Conn, byid = T)
AC = vector()
XYZsum.Conn = as.data.frame(XYZsum.Conn)
XYZsum.Conn$Zone = factor(ifelse(XYZsum.Conn$Zone %in% 
                                c("Habitat Protection", "Conservation Park", "Commonwealth Island (Other)"), "Open", "Closed"))
COTS_Out = COTS_Out[-which(duplicated(COTS_Out)),]
COTS_PA = COTS_Out[-which(duplicated(COTS_PA)),]
### Add PA and OUT variables
XYZsum.Conn = XYZsum.Conn %>% left_join(select(COTS_Out, REEF_NAME, COTS_OUT), by=c("REEF_NAME", "X_COORD", "Y_COORD"))
XYZsum.Conn = XYZsum.Conn %>% left_join(COTS_PA[,c(1,5:7)], by="REEF_NAME")

ACConn = vector()

for(i in 1:length(XYZsum.Conn$COTS_PA)) {
  wj = 1/d[i,]
  yj = XYZsum.Conn$COTS_PA
  wjyj = wj*yj
  AC[i] = sum(wjyj[-i], na.rm = T)
  wj2 = cotsconn.matched[i,]
  yj2 = XYZsum.Conn$COTS_PA
  wjyj2 =wj2*yj2
  ACConn[i] = sum(wjyj2[-i], na.rm =T)
}

XYZsum.Conn$AC = as.vector(AC)
XYZsum.Conn$ACConn = as.vector(ACConn)

XYZsum.Conn = XYZsum.Conn[,c(1,65:70,53:57,2:52,58:64,71:72)]
XYZsum.Conn[7:9] = lapply(XYZsum.Conn[7:9], base::as.factor)
colnames(XYZsum.Conn) = gsub("-", ".", colnames(XYZsum.Conn))
XYZsum.Conn.sc = XYZsum.Conn


XYZsum.Conn.sc[,13:72] = scale(XYZsum.Conn[,13:72])
COTS_PA = XYZsum.Conn[-which(is.na(XYZsum.Conn$COTS_PA)),]
COTS_PA.sc = XYZsum.Conn.sc[-which(is.na(XYZsum.Conn.sc$COTS_PA)),]
COTS_Out = XYZsum.Conn[-which(is.na(XYZsum.Conn$COTS_OUT)),]
COTS_Out.sc = XYZsum.Conn.sc[-which(is.na(XYZsum.Conn.sc$COTS_OUT)),]

```


### Summarise Manta Data as Independent Test Data - Needs Work

```{r}
manta <- read.csv("Data/manta.csv", row.names = NULL, header=T)
colnames(manta)
MantaSum = read.csv("Data/MantaSum.csv", row.names = NULL, header=T)
MantaSum$SAMPLE_DATE = as.character(MantaSum$SAMPLE_DATE)
MantaSum$SAMPLE_DATE = substr(MantaSum$SAMPLE_DATE, 1, nchar(MantaSum$SAMPLE_DATE)-8)
MantaSum$SAMPLE_DATE = as.Date(MantaSum$SAMPLE_DATE, format="%d/%m/%Y")
MantaSum$Year = as.integer(substr(as.character(MantaSum$YEAR_CODE), 1,4))
manta <- manta[,-c(7:8, 11,13,17:39)]

manta.yr <-  MantaSum %>% group_by(REEF_NAME, FULLREEF_ID, A_SECTOR, SHELF, Year) %>%
                      dplyr::mutate(PA = 1*(COTS > 0),
                                Out = 1*(MEAN_COTS>=0.22),
                                Nout = 1*(MEAN_COTS<0.22 & MEAN_COTS>0),
                                OutbeakStatus = ifelse(MEAN_COTS <=0.1, "No Outbreak",
                                                       ifelse(MEAN_COTS <=0.22, "Potential",
                                                              ifelse(MEAN_COTS <=1, "Established", "Severe"))))

# Extract GBRMPA ReefNAme/ID from shapefile

MantaSum$X_LABEL = substr(sub("(.{2})(.*)", "\\1-\\2", MantaSum$FULLREEF_ID), 1,7)
MantaSum = MantaSum %>% inner_join(select(shp@data, X_LABEL, LOC_NAME_L, LABEL_ID), by="X_LABEL")

test.data <- manta.yr %>%
                      group_by(REEF_NAME, FULLREEF_ID, A_SECTOR, SHELF) %>%
                      filter(Year >=2010) %>%
                      dplyr::summarise(N = length(PA),
                                MEAN.CoTS = round(mean(MEAN_COTS),2),
                                SUM = sum(PA),
                                SUM.Out = sum(Out),
                                COTS_PA = 1*(SUM > 0),
                                COTS_OUT = 1*(SUM.Out>0),
                                PA.Nout = 1*(SUM.Out ==0 & sum(Nout)>0)) #%>%
                      #filter()


# Change Reef ID to match GBRMPA format
test.data$ReefID = substr(as.character(test.data$FULLREEF_ID),1,nchar(as.character(test.data$FULLREEF_ID))-1)
test.data$subreef = substr(as.character(test.data$FULLREEF_ID),6,nchar(as.character(test.data$FULLREEF_ID)))
test.data$ReefID = ifelse(test.data$subreef=="S", paste0(substr(test.data$ReefID,1,2), "-", substr(test.data$ReefID,3,5)), 
                          paste0(substr(test.data$ReefID,1,2), "-", substr(test.data$ReefID,3,5), tolower(test.data$subreef)))
colnames(shp@data)[3] = "ReefID"
test.data = test.data %>% 
  inner_join(shp@data[, c(3,14)], by = "ReefID")
colnames(test.data)[14] ="REEF_NAME"
test.data = test.data[c(14,5:11)]
test.data =  test.data %>%
  inner_join(XYZsum, by="REEF_NAME")
# XYZ data
```

### Investigate MultiCollinearity

All looks pretty good

```{r}
X<-COTS_PA[,8:59]
# library(GGally)
# ggpairs(X)
cors = round(cor(X),2)
# library(mctest)
# omcdiag(X,WAGE)
# 
# imcdiag(X,WAGE)
```

### Investigate Spatial Cor 

Need to do this to justify the Autocovariate approch

## Run GBMs

```{r}
# Set Hypothesised Variables
hypoth = c("GBR_BATHY", "GMCS_STRESS_TMN", "CRS_NO3_AV", "CRS_S_AV", "MT_SST_AV", "Primary", "Secondary", "Tertiary", "max.chl", "predHCmaxmean", "w_in_degree", "CROSS_SHELF", "self_seed", "bent.clust", "Zone", "AC", "ACConn", colnames(XYZsum.Conn.sc)[c(65:66, 68,70)]) 
hypoth.renamed = c("Depth", "Stress", "NO3", "Salinity", "SST", "Primary", "Secondary", "Tertiary", "Chl", "Max Coral", "In Degree", "Shelf", "Self Seed", "Coral Comp", "Zone", "AC.Dist", "AC.Conn", "Cyclone", "Bleach", "DHW", "COTS") 
hypoth.all = colnames(COTS_PA.sc)[8:72]
```

### Helper Functions

```{r Helper Functions}
# Relative influence plot
plotinfluence = function(gbmmodel, no.drops=0, ylab=NULL, ylims=F, lims = NULL){
  dat = gbmmodel$contributions
  dat = dat %>% 
    mutate(var = factor(rownames(dat))) %>%
    filter(var %in% hypoth)
  dat$var = hypoth.renamed[match(dat$var,hypoth)]
  dat$var = factor(dat$var, levels = arrange(dat,rel.inf) %>% pull(var))
  dev.expl = (gbmmodel$self.statistics$mean.null-gbmmodel$self.statistics$mean.resid)/gbmmodel$self.statistics$mean.null
  if(!ylims){
  ggplot(dat, aes(y=rel.inf, x=var, fill=var)) +
    #ylim(ylims) +
    geom_bar(stat="identity") +
    scale_fill_manual(values = c(rep("grey", no.drops), rep("seagreen3", length(dat$var)-no.drops))) +
    guides(fill=FALSE) +
    coord_flip() +
    ylab("Relative Importance (%)") + xlab(ylab) +
    geom_label(aes(label = paste0("Deviance Explained = ", round(dev.expl*100,1), "%"), y = max(dat$rel.inf)-6, x=2), 
               fill="White",fontface = "bold")
    # geom_text(data=data.frame(), aes(label = paste0("Deviance Explained = ", round(dev.expl*100,1), "%"), x = -Inf, y = Inf),
    #         hjust = 0, vjust = 1) 
  }else{
  ggplot(dat, aes(y=rel.inf, x=var, fill=var)) +
    ylim(lims) +
    geom_bar(stat="identity") +
    scale_fill_manual(values = c(rep("grey", no.drops), rep("seagreen3", length(dat$var)-no.drops))) +
    guides(fill=FALSE) +
    coord_flip() +
    ylab("Relative Importance (%)") + xlab(ylab) +
    # annotate(geom = 'text', label = paste0("Deviance Explained = ", round(dev.expl*100,1), "%"), y = lims[2]/2, 
    #          x = 2, hjust = -0.3, vjust = 0) +
    geom_label(aes(label = paste0("Deviance Explained = ", round(dev.expl*100,1), "%"), y = lims[2]/2,x=2),
               hjust = 0.3, fill="white", fontface = "bold")
  }
}
plotinfluence(CoTS.gbmOUT, ylims = T, lims= c(0,40))

#k fold cv
k.fold = function(data, k=10, hypoth, family="bernoulli", gbm.y) {
  group = kfold(data, k)
  e = list()
  gbm.y1 = rlang::sym(gbm.y)
  gbm.y1 = enquo(gbm.y1)
  dev = vector("numeric", k)
    for (i in 1:k) {
        train = data[group != i,]
        test = data[group == i,]
        gbmfit = dismo::gbm.step(data=as.data.frame(train),
                        gbm.x = hypoth, gbm.y = gbm.y, # need to add sector and shelf
                        family = family, #what is the family of distribution of our response 
                        learning.rate = 0.001,
                        tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                        bag.fraction=0.5, # how much of the train data do we use for each tree
                        n.trees = 100,
                        max.trees = 100000)
        dev[i] = (gbmfit$self.statistics$mean.null-gbmfit$self.statistics$mean.resid)/gbmfit$self.statistics$mean.null
        # if(gbm.y=="COTS_PA"){
        e[[i]] = evaluate(p=test %>% filter(!!gbm.y1 == T), a= test%>% filter(!!gbm.y1 == F), gbmfit, 
                           n.trees=gbmfit$gbm.call$best.trees, type="response")
        }
  return(list(e, dev))
}

# refit simplified model
refit.simp = function(gbmmodel) {
  gbm.simp = gbm.simplify(gbmmodel)
  vars2drop = which.min(cumsum(gbm.simp$deviance.summary$mean))
  vars = gbm.simp$pred.list[[vars2drop]]
  df = gbmmodel[["gbm.call"]][["dataframe"]]
  distrib = gbmmodel[["distribution"]][["name"]]
  refit = gbm.step(data = df,
                   gbm.x = vars, gbm.y = gbmmodel[["gbm.call"]][["gbm.y"]], family = distrib,
                   tree.complexity = gbmmodel[["interaction.depth"]],
                   learning.rate = gbmmodel[["shrinkage"]],
                   bag.fraction=gbmmodel[["bag.fraction"]], 
                   n.trees = 100,
                   max.trees = 100000
                   )
  return(list(refit,gbm.simp,vars2drop))
}

# Evaluation Statistics
evaluation  = function(gbmmodels) {
  stats = data.frame(model = names(gbmmodels), dev.expl=NA, Training.AUC=NA, CV.AUC=NA, cv.cor=NA, weighted=NA )
  for(i in 1:length(gbmmodels)){
    stats[i,2] = (gbmmodels[[i]]$self.statistics$mean.null-gbmmodels[[i]]$self.statistics$mean.resid)/gbmmodels[[i]]$self.statistics$mean.null
    stats[i,3] = gbmmodels[[i]]$self.statistics$discrimination
    stats[i,4] = gbmmodels[[i]]$cv.statistics$discrimination.mean
    stats[i,5] = gbmmodels[[i]]$cv.statistics$correlation.mean
    stats[i,6] = (stats[i,2] + stats[i,5])/2
    }
  stats = stats %>% arrange(-weighted)
  return(stats)
}

gbm.object=CoTS.gbmPA
n.plots=4

gbm.plot.ggplot = function (gbm.object, variable.no = 0, smooth = FALSE, rug = TRUE, 
    n.plots = length(pred.names), plot.ggplot = T, common.scale = TRUE, write.title = TRUE, 
    y.label = "fitted function", x.label = NULL, show.contrib = TRUE, 
    plot.layout = c(3, 4), ...) 
{
    if (!requireNamespace("gbm")) {
        stop("you need to install the gbm package to run this function")
    }
    requireNamespace("splines")
    gbm.call <- gbm.object$gbm.call
    gbm.x <- gbm.call$gbm.x
    pred.names <- gbm.call$predictor.names
    response.name <- gbm.call$response.name
    data <- gbm.call$dataframe
    max.plots <- plot.layout[1] * plot.layout[2]
    plot.count <- 0
    n.pages <- 1
    if (length(variable.no) > 1) {
        stop("only one response variable can be plotted at a time")
    }
    if (variable.no > 0) {
        n.plots <- 1
    }
    max.vars <- length(gbm.object$contributions$var)
    if (n.plots > max.vars) {
        n.plots <- max.vars
        warning("reducing no of plotted predictors to maximum available (", 
            max.vars, ")")
    }
    predictors <- list(rep(NA, n.plots))
    responses <- list(rep(NA, n.plots))
    for (j in c(1:n.plots)) {
        if (n.plots == 1) {
            k <- variable.no
        }
        else {
            k <- match(gbm.object$contributions$var[j], pred.names)
        }
        if (is.null(x.label)) {
            var.name <- gbm.call$predictor.names[k]
        }
        else {
            var.name <- x.label
        }
        pred.data <- data[, gbm.call$gbm.x[k]]
        response.matrix <- gbm::plot.gbm(gbm.object, k, return.grid = TRUE)
        predictors[[j]] <- response.matrix[, 1]
        if (is.factor(data[, gbm.call$gbm.x[k]])) {
            predictors[[j]] <- factor(predictors[[j]], levels = levels(data[, 
                gbm.call$gbm.x[k]]))
        }
        responses[[j]] <- response.matrix[, 2] - mean(response.matrix[, 
            2])
        if (j == 1) {
            ymin = min(responses[[j]])
            ymax = max(responses[[j]])
        }
        else {
            ymin = min(ymin, min(responses[[j]]))
            ymax = max(ymax, max(responses[[j]]))
        }
    }
    if(plot.ggplot == F){
    op <- graphics::par(no.readonly = TRUE)
    graphics::par(mfrow = plot.layout)
    for (j in c(1:n.plots)) {
        if (plot.count == max.plots) {
            plot.count = 0
            n.pages <- n.pages + 1
        }
        plot.count <- plot.count + 1
        if (n.plots == 1) {
            k <- match(pred.names[variable.no], gbm.object$contributions$var)
            if (show.contrib) {
                x.label <- paste(var.name, "  (", round(gbm.object$contributions[k, 
                  2], 1), "%)", sep = "")
            }
        }
        else {
            k <- match(gbm.object$contributions$var[j], pred.names)
            var.name <- gbm.call$predictor.names[k]
            if (show.contrib) {
                x.label <- paste(var.name, "  (", round(gbm.object$contributions[j, 
                  2], 1), "%)", sep = "")
            }
            else x.label <- var.name
        }
        if (common.scale) {
            plot(predictors[[j]], responses[[j]], ylim = c(ymin, 
                ymax), type = "l", xlab = x.label, ylab = y.label, 
                ...)
        }
        else {
            plot(predictors[[j]], responses[[j]], type = "l", 
                xlab = x.label, ylab = y.label, ...)
        }
        if (smooth & is.vector(predictors[[j]])) {
            temp.lo <- loess(responses[[j]] ~ predictors[[j]], 
                span = 0.3)
            lines(predictors[[j]], fitted(temp.lo), lty = 2, 
                col = 2)
        }
        if (plot.count == 1) {
            if (write.title) {
                title(paste(response.name, " - page ", n.pages, 
                  sep = ""))
            }
            if (rug & is.vector(data[, gbm.call$gbm.x[variable.no]])) {
                rug(quantile(data[, gbm.call$gbm.x[variable.no]], 
                  probs = seq(0, 1, 0.1), na.rm = TRUE))
            }
        }
        else {
            if (write.title & j == 1) {
                title(response.name)
            }
            if (rug & is.vector(data[, gbm.call$gbm.x[k]])) {
                rug(quantile(data[, gbm.call$gbm.x[k]], probs = seq(0, 
                  1, 0.1), na.rm = TRUE))
            }
        }
    }
    graphics::par(op)
    }
      k1 <- match(gbm.object$contributions$var[1:n.plots], pred.names)
      var.name1 <- gbm.call$predictor.names[k1]
      var.name2 <- hypoth.renamed[match(var.name1, hypoth)]
      predictordf <- data[, var.name1] %>% `colnames<-` (var.name2) %>%
        tidyr::gather(key="Variable", value = "Pred") %>%
        mutate(Variable = factor(Variable, levels = var.name2))
      names(predictors) <- var.name2; names(responses) <- var.name2
      responses1 = data.frame(matrix(unlist(responses), nrow = length(responses[[1]]), byrow=F), stringsAsFactors = F)
      predictors1 = data.frame(matrix(unlist(predictors), nrow = length(predictors[[1]]), byrow=F), stringsAsFactors = F)
      colnames(responses1) = colnames(predictors1) = var.name2
      responses1 = responses1 %>% tidyr::gather(key="Variable", value = "Resp.Value") %>% 
        mutate(Variable = factor(Variable, levels = var.name2))
      predictors1 = predictors1 %>% tidyr::gather(key="Variable", value = "Pred.Value")
      df = cbind(responses1, predictors1[,2]); colnames(df)[3] = "Pred.Value"
      ggplot(df, aes(x=Pred.Value, y=Resp.Value)) + geom_line() + 
        geom_rug(data = predictordf, aes(x=Pred, y=NULL)) +
        facet_wrap(~Variable,scales = "free") + ylab(y.label) + xlab("Predictor") #+
        
    
}

```

### Presence Absence

```{r Presence Model}
# 1. Presence Absence Model ----
CoTS.gbmPA <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_PA.sc)),
                gbm.x = hypoth[-which(hypoth %in% c("AC", "ACConn"))], gbm.y = "COTS_PA", # need to add sector and shelf
                family = "bernoulli", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmPA.AC <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_PA.sc)),
                gbm.x = hypoth[-which(hypoth %in% "ACConn")], gbm.y = "COTS_PA", # need to add sector and shelf
                family = "bernoulli", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmPA.ACConn <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_PA.sc)),
                gbm.x = hypoth[-which(hypoth %in% "AC")], gbm.y = "COTS_PA", # need to add sector and shelf
                family = "bernoulli", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmPA.ALL <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_PA.sc)),
                gbm.x = hypoth, gbm.y = "COTS_PA", # need to add sector and shelf
                family = "bernoulli", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmPA.allenv <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_PA.sc)),
                gbm.x = hypoth.all[-which(hypoth %in% c("AC", "ACConn"))], gbm.y = "COTS_PA", # need to add sector and shelf
                family = "bernoulli", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
evaluation(gbmmodels = list(PA_Hypoth = CoTS.gbmPA, PA_AC = CoTS.gbmPA.AC, PA_ACConn = CoTS.gbmPA.ACConn, 
                            PA_All = CoTS.gbmPA.ALL, PA_Allenv = CoTS.gbmPA.allenv))

# Partial Dependency Plots
layout(matrix(c(1,2),nrow=1), widths=c(2,1))
gbm.plot(CoTS.gbmPA, plot.ggplot = T, n.plots=6, write.title = F, plot.layout = c(3,2))
gbm.plot(CoTS.gbmPA.AC, n.plots=6, write.title = F, plot.layout = c(3,2))
gbm.plot(CoTS.gbmPA.ACConn, n.plots=6, write.title = F, plot.layout = c(3,2))
gbm.plot(CoTS.gbmPA.ALL, n.plots=6, write.title = F, plot.layout = c(3,2))
CoTS.gbmPA.allenv$contributions

# Interactions
CoTS.gbmPA.int = dismo::gbm.interactions(CoTS.gbmPA)
CoTS.gbmPAAC.int = dismo::gbm.interactions(CoTS.gbmPA.AC)
CoTS.gbmPAACConn.int = dismo::gbm.interactions(CoTS.gbmPA.ACConn)

#Simplify Models
CoTS.gbmPA.Simp = refit.simp(CoTS.gbmPA)
CoTS.gbmPA_AC.Simp = refit.simp(CoTS.gbmPA.AC)
CoTS.gbmPA_ACConn.Simp = refit.simp(CoTS.gbmPA.ACConn)
CoTS.gbmPA_All.Simp = refit.simp(CoTS.gbmPA.ALL)

# plot(1:19, cumsum(PA.Simp$deviance.summary$mean))
# plot(1:19, cumsum(PA_AC.Simp$deviance.summary$mean))
# plot(1:17, cumsum(PA_ACConn.Simp$deviance.summary$mean))

# Plot Outputs
(gbmplotPA = plotinfluence(CoTS.gbmPA, no.drops=CoTS.gbmPA.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotPA.AC = plotinfluence(CoTS.gbmPA.AC, no.drops =CoTS.gbmPA_AC.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotPA.ACConn = plotinfluence(CoTS.gbmPA.ACConn, no.drops = CoTS.gbmPA_ACConn.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotPA.All = plotinfluence(CoTS.gbmPA.ALL, no.drops = CoTS.gbmPA_All.Simp[[3]], ylims = T, lims = c(0,40)))

evalsPA = evaluation(gbmmodels = list(PA_NoAC = CoTS.gbmPA, PA_NoAC.Simp = CoTS.gbmPA.Simp[[1]], 
                            PA_AC = CoTS.gbmPA.AC, PA_AC.Simp = CoTS.gbmPA_AC.Simp[[1]],
                            PA_ACConn = CoTS.gbmPA.ACConn, PA_ACConn.Simp = CoTS.gbmPA_ACConn.Simp[[1]],
                            PA_All = CoTS.gbmPA.ALL, PA_All.Simp = CoTS.gbmPA_All.Simp[[1]]))
evalsPA
# Mean Prediciton Error
data <- COTS_PA.sc
n <- dim(data)[1]
all.error <- rep(0,10)
all.errorACConn <- rep(0,10)
all.errorAC <- rep(0,10)
plot(NA, xlim=c(0,1), ylim=c(0,1), xlab="Obs", ylab="Pred")

for (i in 1:10) {
    random=sampling::srswor(round(n/10),n)                     # simple random sampling without replacement of 10% of the data
    bootstrap= subset(cbind(random,data), random==0)                           # 90% of the initial data is used to calibrate the model
    new.data= subset(cbind(random,data), random==1)                            # 10% of the initial data is used to compare model predictions vs observations
    gbm.CV <- gbm.fixed(data=bootstrap, family="bernoulli", 
                        gbm.x = hypoth[-which(hypoth %in% c("AC", "ACConn"))], gbm.y = "COTS_PA",
                        learning.rate = 0.01, tree.complexity = 2, n.trees = 800)
    pred <- predict.gbm(gbm.CV, new.data, n.trees=800, type="response")
    error <- 100 * abs(pred-new.data$COTS_PA)
    all.error[i] <- mean(error)
    gbm.CV.AC <- gbm.fixed(data=bootstrap, family="bernoulli", 
                        gbm.x = hypoth[-which(hypoth %in% "ACConn")], gbm.y = "COTS_PA",
                        learning.rate = 0.01, tree.complexity = 2, n.trees = 800)
    pred <- predict.gbm(gbm.CV.AC, new.data, n.trees=800, type="response")
    error <- 100 * abs(pred-new.data$COTS_PA)
    #points(pred,new.data$COTS_PA, pch=19, col="black")
    all.errorAC[i] <- mean(error)
    gbm.CV.ACConn <- gbm.fixed(data=bootstrap, family="bernoulli", 
                        gbm.x = hypoth[-which(hypoth %in% "AC")], gbm.y = "COTS_PA",
                        learning.rate = 0.01, tree.complexity = 2, n.trees = 800)
    pred <- predict.gbm(gbm.CV.ACConn, new.data, n.trees=800, type="response")
    error <- 100 * abs(pred-new.data$COTS_PA)
    #points(pred,new.data$COTS_PA, pch=19, col="black")
    all.errorACConn[i] <- mean(error)
    }
mean(all.error)      
mean(all.errorAC)
mean(all.errorACConn)
```

### Persistence

```{r Persistence Model}
# 2. Persistence Model ----
COTS_PA.sc.NOZERO = COTS_PA.sc[-which(COTS_PA.sc$COTS_PROP==0),]
CoTS.gbmPROPNOZERO <- dismo::gbm.step(data=as.data.frame(COTS_PA.sc.NOZERO),
                gbm.x = hypoth[-which(hypoth %in% c("AC", "ACConn"))], gbm.y = "COTS_PROP", # need to add sector and shelf
                family = "poisson", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmPROPNOZERO.AC <- dismo::gbm.step(data=as.data.frame(COTS_PA.sc.NOZERO),
                gbm.x = hypoth[-which(hypoth %in% "ACConn")], gbm.y = "COTS_PROP", # need to add sector and shelf
                family = "poisson", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmPROPNOZERO.ACConn <- dismo::gbm.step(data=as.data.frame(COTS_PA.sc.NOZERO),
                gbm.x = hypoth[-which(hypoth %in% "AC")], gbm.y = "COTS_PROP", # need to add sector and shelf
                family = "poisson", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmPROPNOZERO.All <- dismo::gbm.step(data=as.data.frame(COTS_PA.sc.NOZERO),
                gbm.x = hypoth, gbm.y = "COTS_PROP", # need to add sector and shelf
                family = "poisson", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)



# Partial Dependency Plots
gbm.plot(CoTS.gbmPROPNOZERO, n.plots=9, write.title = F, plot.layout = c(3,3))
gbm.plot(CoTS.gbmPROPNOZERO.AC, n.plots=9, write.title = F, plot.layout = c(3,3))
gbm.plot(CoTS.gbmPROPNOZERO.ACConn, n.plots=9, write.title = F, plot.layout = c(3,3))
gbm.plot(CoTS.gbmPROPNOZERO.All, n.plots=9, write.title = F, plot.layout = c(3,3))

# Interactions
CoTS.gbmPROPNOZERO.int = dismo::gbm.interactions(CoTS.gbmPROPNOZERO)
CoTS.gbmPROPNOZERO.AC.int = dismo::gbm.interactions(CoTS.gbmPROPNOZERO.AC)
CoTS.gbmPROPNOZERO.ACConn.int = dismo::gbm.interactions(CoTS.gbmPROPNOZERO.ACConn)
CoTS.gbmPROPNOZERO.All.int = dismo::gbm.interactions(CoTS.gbmPROPNOZERO.All)

#Simplify Models
PROPNOZERO.Simp = refit.simp(CoTS.gbmPROPNOZERO) # 4
PROPNOZERO_AC.Simp = refit.simp(CoTS.gbmPROPNOZERO.AC)
PROPNOZERO_ACConn.Simp = refit.simp(CoTS.gbmPROPNOZERO.ACConn)
PROPNOZERO_All.Simp = refit.simp(CoTS.gbmPROPNOZERO.All)

# Plot Influence
(gbmplotPROPNOZERO = plotinfluence(CoTS.gbmPROPNOZERO, PROPNOZERO.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotPROPNOZERO.AC = plotinfluence(CoTS.gbmPROPNOZERO.AC, no.drops = PROPNOZERO_AC.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotPROPNOZERO.ACConn = plotinfluence(CoTS.gbmPROPNOZERO.ACConn, no.drops = PROPNOZERO_ACConn.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotPROPNOZERO.All = plotinfluence(CoTS.gbmPROPNOZERO.All, no.drops = PROPNOZERO_All.Simp[[3]], ylims = T, lims = c(0,40)))

# Evalutation
evalsPROPN0 = evaluation(gbmmodels = list(PROPN0_NoAC = CoTS.gbmPROPNOZERO, PROPN0_NoAC.Simp = PROPNOZERO.Simp[[1]], 
                            PROPN0_AC = CoTS.gbmPROPNOZERO.AC, PROPN0_AC.Simp = PROPNOZERO_AC.Simp[[1]],
                            PROPN0_ACConn = CoTS.gbmPROPNOZERO.ACConn, PROPN0_ACConn.Simp = PROPNOZERO_ACConn.Simp[[1]],
                            PROPN0_All = CoTS.gbmPROPNOZERO.All, PROPN0_All.Simp = PROPNOZERO_All.Simp[[1]]))


```

### Persistence Model - with Zero's

```{r Persistance Model}
# Include Zero
CoTS.gbmPROP <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_PA.sc)),
                gbm.x = hypoth[-which(hypoth %in% c("AC", "ACConn"))], gbm.y = "COTS_PROP", # need to add sector and shelf
                family = "poisson", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmPROP.AC <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_PA.sc)),
                gbm.x = hypoth[-which(hypoth %in% "ACConn")], gbm.y = "COTS_PROP", # need to add sector and shelf
                family = "poisson", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmPROP.ACConn <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_PA.sc)),
                gbm.x = hypoth[-which(hypoth %in% "AC")], gbm.y = "COTS_PROP", # need to add sector and shelf
                family = "poisson", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmPROP.All <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_PA.sc)),
                gbm.x = hypoth, gbm.y = "COTS_PROP", # need to add sector and shelf
                family = "poisson", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)

# Partial Dependency Plots
gbm.plot(CoTS.gbmPROP, n.plots=9, write.title = F, plot.layout = c(3,3))
gbm.plot(CoTS.gbmPROP.AC.AC, n.plots=9, write.title = F, plot.layout = c(3,3))
gbm.plot(CoTS.gbmPROP.ACConn, n.plots=9, write.title = F, plot.layout = c(3,3))
gbm.plot(CoTS.gbmPROP.All, n.plots=9, write.title = F, plot.layout = c(3,3))

# Interactions
CoTS.gbmPROP.int = dismo::gbm.interactions(CoTS.gbmPROP)
CoTS.gbmPROP.AC.int = dismo::gbm.interactions(CoTS.gbmPROP.AC)
CoTS.gbmPROP.ACConn.int = dismo::gbm.interactions(CoTS.gbmPROP.ACConn)
CoTS.gbmPROP.All.int = dismo::gbm.interactions(CoTS.gbmPROP.All)

#Simplify Models
PROP.Simp = refit.simp(CoTS.gbmPROP) # 4
PROP_AC.Simp = refit.simp(CoTS.gbmPROP.AC)
PROP_ACConn.Simp = refit.simp(CoTS.gbmPROP.ACConn)
PROP_All.Simp = refit.simp(CoTS.gbmPROP.All)

# Plot Influence
(gbmplotPROP = plotinfluence(CoTS.gbmPROP, PROP.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotPROP.AC = plotinfluence(CoTS.gbmPROP.AC, no.drops = PROP_AC.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotPROP.ACConn = plotinfluence(CoTS.gbmPROP.ACConn, no.drops = PROP_ACConn.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotPROP.All = plotinfluence(CoTS.gbmPROP.All, no.drops = PROP_All.Simp[[3]], ylims = T, lims = c(0,40)))

evalsPROP = evaluation(gbmmodels = list(PROP_NoAC = CoTS.gbmPROP, PROP_NoAC.Simp = PROP.Simp[[1]], 
                            PROP_AC = CoTS.gbmPROP.AC, PROP_AC.Simp = PROP_AC.Simp[[1]],
                            PROP_ACConn = CoTS.gbmPROP.ACConn, PROP_ACConn.Simp = PROP_ACConn.Simp[[1]],
                            PROP_All = CoTS.gbmPROP.All, PROP_All.Simp = PROP_All.Simp[[1]]))

# Mean Prediciton Error
data <- COTS_PA.sc
n <- dim(data)[1]
all.error <- rep(0,10)
all.errorACConn <- rep(0,10)
all.errorAC <- rep(0,10)
# plot(NA, xlim=c(0,1), ylim=c(0,1), xlab="Obs", ylab="Pred")

for (i in 1:10) {
    random=sampling::srswor(round(n/10),n)                     # simple random sampling without replacement of 10% of the data
    bootstrap= subset(cbind(random,data), random==0)                           # 90% of the initial data is used to calibrate the model
    new.data= subset(cbind(random,data), random==1)                            # 10% of the initial data is used to compare model predictions vs observations
    gbm.CV <- gbm.fixed(data=bootstrap, family="poisson", 
                        gbm.x = hypoth[-which(hypoth %in% c("AC", "ACConn"))], gbm.y = "COTS_PROP",
                        learning.rate = 0.01, tree.complexity = 2, n.trees = 800)
    pred <- predict.gbm(gbm.CV, new.data, n.trees=800, type="response")
    error <- 100 * abs(pred-new.data$COTS_PA)/new.data$COTS_PA
    all.error[i] <- mean(error)
    gbm.CV.AC <- gbm.fixed(data=bootstrap, family="poisson", 
                        gbm.x = hypoth[-which(hypoth %in% "ACConn")], gbm.y = "COTS_PROP",
                        learning.rate = 0.01, tree.complexity = 2, n.trees = 800)
    pred <- predict.gbm(gbm.CV.AC, new.data, n.trees=800, type="response")
    error <- 100 * abs(pred-new.data$COTS_PA)
    #points(pred,new.data$COTS_PA, pch=19, col="black")
    all.errorAC[i] <- mean(error)
    gbm.CV.ACConn <- gbm.fixed(data=bootstrap, family="poisson", 
                        gbm.x = hypoth[-which(hypoth %in% "AC")], gbm.y = "COTS_PROP",
                        learning.rate = 0.01, tree.complexity = 2, n.trees = 800)
    pred <- predict.gbm(gbm.CV.ACConn, new.data, n.trees=800, type="response")
    error <- 100 * abs(pred-new.data$COTS_PA)
    #points(pred,new.data$COTS_PA, pch=19, col="black")
    all.errorACConn[i] <- mean(error)
    }
mean(all.error)      
mean(all.errorAC)
mean(all.errorACConn)

```

### Outbreaks 

```{r Outbreak Model}
# 3. COTS Outbreak Model ----

CoTS.gbmOUT <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_Out.sc)),
                gbm.x = hypoth[-which(hypoth %in% c("AC", "ACConn"))], gbm.y = "COTS_OUT", # need to add sector and shelf
                family = "bernoulli", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmOUT.AC <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_Out.sc)),
                gbm.x = hypoth[-which(hypoth %in% "ACConn")], gbm.y = "COTS_OUT", # need to add sector and shelf
                family = "bernoulli", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmOUT.ACConn <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_Out.sc)),
                gbm.x = hypoth[-which(hypoth %in% "AC")], gbm.y = "COTS_OUT", # need to add sector and shelf
                family = "bernoulli", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)
CoTS.gbmOUT.All <- dismo::gbm.step(data=na.omit(as.data.frame(COTS_Out.sc)),
                gbm.x = hypoth, gbm.y = "COTS_OUT", # need to add sector and shelf
                family = "bernoulli", #what is the family of distribution of our response 
                learning.rate = 0.001,
                tree.complexity = 3, #how "deep" are each tree, i.e how complex are the interactions 3 is for two
                bag.fraction=0.5, # how much of the train data do we use for each tree
                n.trees = 100,
                max.trees = 100000)

# Partial Dependency Plots
gbm.plot(CoTS.gbmOUT, n.plots=9, write.title = F, plot.layout = c(3,3))
gbm.plot(CoTS.gbmOUT.AC.AC, n.plots=9, write.title = F, plot.layout = c(3,3))
gbm.plot(CoTS.gbmOUT.ACConn, n.plots=9, write.title = F, plot.layout = c(3,3))
gbm.plot(CoTS.gbmOUT.All, n.plots=9, write.title = F, plot.layout = c(3,3))

# Interactions
CoTS.gbmOUT.int = dismo::gbm.interactions(CoTS.gbmOUT)
CoTS.gbmOUT.AC.int = dismo::gbm.interactions(CoTS.gbmOUT.AC)
CoTS.gbmOUT.ACConn.int = dismo::gbm.interactions(CoTS.gbmOUT.ACConn)
CoTS.gbmOUT.All.int = dismo::gbm.interactions(CoTS.gbmOUT.All)

#Simplify Models
OUT.Simp = refit.simp(CoTS.gbmOUT) 
OUT_AC.Simp = refit.simp(CoTS.gbmOUT.AC)
OUT_ACConn.Simp = refit.simp(CoTS.gbmOUT.ACConn)
OUT_All.Simp = refit.simp(CoTS.gbmOUT.All)
PROPNOZERO_ACConn.Simp
# Plot Influence
(gbmplotOUT = plotinfluence(CoTS.gbmOUT, OUT.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotOUT.AC = plotinfluence(CoTS.gbmOUT.AC, no.drops = OUT_AC.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotOUT.ACConn = plotinfluence(CoTS.gbmOUT.ACConn, no.drops = OUT_ACConn.Simp[[3]], ylims = T, lims = c(0,40)))
(gbmplotOUT.All = plotinfluence(CoTS.gbmOUT.All, no.drops = OUT_All.Simp[[3]], ylims = T, lims = c(0,40)))

evalsOUT = evaluation(gbmmodels = list(OUT_NoAC = CoTS.gbmOUT, OUT_NoAC.Simp = OUT.Simp[[1]], 
                            OUT_AC = CoTS.gbmOUT.AC, OUT_AC.Simp = OUT_AC.Simp[[1]],
                            OUT_ACConn = CoTS.gbmOUT.ACConn, OUT_ACConn.Simp = OUT_ACConn.Simp[[1]],
                            OUT_All = CoTS.gbmOUT.All, OUT_All.Simp = OUT_All.Simp[[1]]))
write.csv(x=bind_rows(evalsPA,evalsPROPN0,evalsPROPN0, evalsOUT), file = "ModelEvaluation/modelevals.csv",row.names = F)
```


```{r Arrange Plots}
# Best Models
gridExtra::grid.arrange(
  gbmplotPA.AC + theme(axis.title.x=element_blank()) + xlab("Presence"), 
  gbmplotPROPNOZERO.ACConn + theme(axis.title.x=element_blank()) +xlab("Pervasiveness") , gbmplotOUT.ACConn + xlab("Outbreak"),
  gbm.plot.ggplot(CoTS.gbmPA.AC, plot.ggplot = T, n.plots = 4, y.label = NULL) + 
    theme(axis.title.x=element_blank()),
  gbm.plot.ggplot(CoTS.gbmPROPNOZERO.ACConn, plot.ggplot = T, n.plots = 4,y.label = NULL) + 
    theme(axis.title.x=element_blank()),
  gbm.plot.ggplot(CoTS.gbmOUT.ACConn, plot.ggplot = T, n.plots = 4, y.label = NULL), 
  layout_matrix = matrix(1:6,3,2,byrow = F))

# Presence
gridExtra::grid.arrange(
  gbmplotPA + theme(axis.title.x=element_blank()) + xlab("Presence"), 
  gbmplotPA.AC + theme(axis.title.x=element_blank()) +xlab("Presence") , gbmplotPA.ACConn + xlab("Presence"),
  gbm.plot.ggplot(CoTS.gbmPA, plot.ggplot = T, n.plots = 4, y.label = NULL) + 
    theme(axis.title.x=element_blank()),
  gbm.plot.ggplot(CoTS.gbmPA.AC, plot.ggplot = T, n.plots = 4,y.label = NULL) + 
    theme(axis.title.x=element_blank()),
  gbm.plot.ggplot(CoTS.gbmPA.ACConn, plot.ggplot = T, n.plots = 4, y.label = NULL), 
  layout_matrix = matrix(1:6,3,2,byrow = F))

# Pervasive No Zero
gridExtra::grid.arrange(
  gbmplotPROPNOZERO + theme(axis.title.x=element_blank()) + xlab("Pervasiveness (no 0)"), 
  gbmplotPROPNOZERO.AC + theme(axis.title.x=element_blank()) +xlab("Pervasiveness (no 0)") , 
  gbmplotPROPNOZERO.ACConn + xlab("Pervasiveness (no 0)"),
  gbm.plot.ggplot(CoTS.gbmPROPNOZERO, plot.ggplot = T, n.plots = 4, y.label = NULL) + 
    theme(axis.title.x=element_blank()),
  gbm.plot.ggplot(CoTS.gbmPROPNOZERO.AC, plot.ggplot = T, n.plots = 4,y.label = NULL) + 
    theme(axis.title.x=element_blank()),
  gbm.plot.ggplot(CoTS.gbmPROPNOZERO.ACConn, plot.ggplot = T, n.plots = 4, y.label = NULL), 
  layout_matrix = matrix(1:6,3,2,byrow = F))

# Pervasive
gridExtra::grid.arrange(
  gbmplotPROP + theme(axis.title.x=element_blank()) + xlab("Pervasiveness")+ylim(c(0,50)), 
  gbmplotPROP.AC + theme(axis.title.x=element_blank()) +xlab("Pervasiveness (AC.Dist)") +ylim(c(0,50)), 
  gbmplotPROP.ACConn + xlab("Pervasiveness (AC.Conn)")+ylim(c(0,50)),
  gbm.plot.ggplot(CoTS.gbmPROP, plot.ggplot = T, n.plots = 4, y.label = NULL) + 
    theme(axis.title.x=element_blank()),
  gbm.plot.ggplot(CoTS.gbmPROP.AC, plot.ggplot = T, n.plots = 4,y.label = NULL) + 
    theme(axis.title.x=element_blank()),
  gbm.plot.ggplot(CoTS.gbmPROP.ACConn, plot.ggplot = T, n.plots = 4, y.label = NULL), 
  layout_matrix = matrix(1:6,3,2,byrow = F))

# Outbreaks
gridExtra::grid.arrange(
  gbmplotOUT + theme(axis.title.x=element_blank()) + xlab("Outbreak"), 
  gbmplotOUT.AC + theme(axis.title.x=element_blank()) +xlab("Outbreak (AC.Dist)")+ylim(c(0,70)) , 
  gbmplotOUT.ACConn + xlab("Outbreak (AC.Conn)"),
  gbm.plot.ggplot(CoTS.gbmOUT, plot.ggplot = T, n.plots = 4, y.label = NULL) + 
    theme(axis.title.x=element_blank()),
  gbm.plot.ggplot(CoTS.gbmOUT.AC, plot.ggplot = T, n.plots = 4,y.label = NULL) + 
    theme(axis.title.x=element_blank()),
  gbm.plot.ggplot(CoTS.gbmOUT.ACConn, plot.ggplot = T, n.plots = 4, y.label = NULL), 
  layout_matrix = matrix(1:6,3,2,byrow = F))

```

## Validation

### Model Evaluation

```{r Validation}
evalsOUT = evaluation(gbmmodels = list(OUT_NoAC = CoTS.gbmOUT, OUT_NoAC.Simp = OUT.Simp[[1]], 
                            OUT_AC = CoTS.gbmOUT.AC, OUT_AC.Simp = OUT_AC.Simp[[1]],
                            OUT_ACConn = CoTS.gbmOUT.ACConn, OUT_ACConn.Simp = OUT_ACConn.Simp[[1]],
                            OUT_All = CoTS.gbmOUT.All, OUT_All.Simp = OUT_All.Simp[[1]]))
write.csv(x=bind_rows(evalsPA,evalsPROPN0,evalsPROPN0, evalsOUT), file = "ModelEvaluation/modelevals.csv",row.names = F)

# #### Validate against manta data
# test.data$predsPA = gbm::predict.gbm(CoTS.gbmPA, test.data, n.trees=CoTS.gbmPA$gbm.call$best.trees, type="response")
# test.data$predsOUT = gbm::predict.gbm(CoTS.gbmOUT, test.data, n.trees=CoTS.gbmOUT$gbm.call$best.trees, type="response")
# p = test.data %>% filter(COTS_PA == T) %>% pull(predsPA)
# a = test.data%>% filter(COTS_PA == F) %>% pull(predsPA)
# e1 = evaluate(p=p, a=a, model = CoTS.gbmPA)
# e1
# p = test.data %>% filter(COTS_OUT == T) %>% pull(predsOUT)
# a = test.data %>% filter(COTS_OUT == F) %>% pull(predsOUT)
# e2 = evaluate(p=p, a=a, model = CoTS.gbmOUT)
# e2
# # test.data$predsPA.log = ifelse(test.data$predsPA > 0.32,1,0)
# # test.data$predsOUT.log = ifelse(test.data$predsOUT > 0.18,1,0)
# # test.data$CORRECT = ifelse(test.data$COTS_PA==test.data$predsPA.log, 1, 0)
# # test.data$CORRECT.OUT = ifelse(test.data$COTS_OUT==test.data$predsOUT.log, 1, 0)
# 
# ggplot(test.data, aes(x = predsPA, y = as.numeric(COTS_PA))) +
#         geom_point(shape = 21) +
#         geom_smooth(method='glm', method.args = list(family = "binomial"),color = "black", fill="steelblue") +
#         ggtitle("BRT Validation") +
#         labs(x = "P(COTS Outbeaks)", y = "COTS Outbreaks Observed") +
#         theme_light()
# ggplot(test.data, aes(x = predsOUT, y = as.numeric(COTS_OUT))) +
#         geom_point(shape = 21) +
#         geom_smooth(method='glm', method.args = list(family = "binomial"),color = "black", fill="steelblue") +
#         ggtitle("BRT Validation") +
#         labs(x = "P(COTS Outbeaks)", y = "COTS Outbreaks Observed") +
#         theme_light()
```


### Predictions

### First I need to scale the predictors back

```{r}
preds.CoTSPA <- gbm::predict.gbm(CoTS.gbmPA, XYZsum, n.trees=CoTS.gbmPA$gbm.call$best.trees, type="response")
hist(preds.CoTSPA)
preds.CoTSPA.sc <- gbm::predict.gbm(CoTS.gbmPA.sc, XYZsum, n.trees=CoTS.gbmPA.sc$gbm.call$best.trees, type="response")
hist(preds.CoTSPA)

preds.CoTSOUT <- gbm::predict.gbm(CoTS.gbmOUT, XYZsum, n.trees=CoTS.gbmOUT$gbm.call$best.trees, type="response")
hist(preds.CoTSOUT)

shpdata = shp@data %>% dplyr::select(LOC_NAME_L, X_COORD, Y_COORD) 
colnames(shpdata)[1] = "ReefName"

dir.create("Predictions")
preds.PA <- data.frame(XYZsum[1], preds.CoTSPA) %>% inner_join(shpdata, by="ReefName")
write.csv(preds.PA, "Predictions/preds_PA.csv", row.names = F)
preds.OUT <- data.frame(XYZsum[1], preds.CoTSOUT) %>% inner_join(shpdata, by="ReefName")
write.csv(preds.OUT, "Predictions/preds_OUT.csv", row.names = F)
write.csv(COTS_PA  %>% inner_join(shpdata, by="ReefName"), "Predictions/Training.csv", row.names = F)
coordinates(preds.PA) <- ~X_COORD+Y_COORD

write.csv(test.data %>% inner_join(shpdata, by="ReefName") %>% na.omit(), "Predictions/preds_manta.csv", row.names = F)

# gridded(preds.PA)<- TRUE
# preds.PA <- raster(preds.PA["preds.CoTSPA"])
# plot(preds.PA)
# writeRaster(preds.PA, "Predicitons/preds_PA.tif", format="GTiff", overwrite=T)

```


## Bayesian Hierarchical Model

Here we will implement a bayesian hierarchical model to predict the probability of an outbreak on reefs on the GBR

*Accounting for multi-scale spatial autocorrelation improves performance of invasive species distribution modelling (iSDM)*

```{r}
library(brms)
library(rstan)
library(rstanarm)
library(coda)
library(bayesplot)
library(ggplot2)
library(broom)
library(tidyr)
library(lme4)
library(MASS)
library(loo)
library(pROC)
library(caret)

COTS_PA.sc = COTS_PA.sc %>% select(-COTS_OUT)
#### RStanARM ----
ploteffects.bent = function(model) {
summy = as.data.frame(model$stan_summary)
#browser()
summy = summy %>% 
  mutate(Variable = factor(rownames(summy))) %>%
  filter(Variable %in% c(hypoth, "ZoneOpen", levels(Variable)[grep("^bent", levels(Variable))])) %>%
  mutate(Sig = ifelse(sign(`2.5%`)==sign(`97.5%`), "Y", "N"))
summy$Variable = factor(summy$Variable, levels = arrange(summy,mean) %>% pull(Variable))
#browser()
ggplot(summy,aes(x=Variable,y=mean))  + 
  geom_hline(yintercept = 0) +
  geom_linerange(aes(ymin=`25%`,ymax=`75%`), size=1.5) +
  geom_linerange(aes(ymin=`2.5%`,ymax=`97.5%`)) +
  geom_point(size=3, aes(colour=Sig)) + 
  scale_color_manual(values = c("grey","black")) +
  guides(colour=FALSE) +
  theme(plot.title = element_blank(),
                axis.title.x = element_blank(),
                axis.title.y = element_blank()) +
  coord_flip() 
}

ploteffects = function(model) {
summy = as.data.frame(model$stan_summary)
#browser()
summy = summy %>% 
  mutate(Variable = factor(rownames(summy))) %>%
  filter(Variable %in% c(hypoth, "ZoneOpen")) %>%
  mutate(Sig = ifelse(sign(`2.5%`)==sign(`97.5%`), "Y", "N"))
summy$Variable = factor(summy$Variable, levels = arrange(summy,mean) %>% pull(Variable))
#browser()
ggplot(summy,aes(x=Variable,y=mean))  + 
  geom_hline(yintercept = 0) +
  geom_linerange(aes(ymin=`25%`,ymax=`75%`), size=1.5) +
  geom_linerange(aes(ymin=`2.5%`,ymax=`97.5%`)) +
  geom_point(size=3, aes(colour=Sig)) + 
  scale_color_manual(values = c("grey","black")) +
  guides(colour=FALSE) +
  theme(plot.title = element_blank(),
                axis.title.x = element_blank(),
                axis.title.y = element_blank()) +
  coord_flip() 
}


AUC = function(bayesmodel) {
    fit1=bayesmodel
    # Predicted probabilities
    linpred <- posterior_linpred(fit1)
    preds <- posterior_linpred(fit1, transform=TRUE)
    pred <- colMeans(preds)
    pr <- as.integer(pred >= 0.5)
    # confusion matrix
    roc(fit1$y,pred,percent=TRUE,col="#1c61b6",  print.auc=TRUE)$auc[1]
    # posterior classification accuracy
}

k.foldAUC = function(mod, kfoldmod) {
    fit1 = mod
    cv.auc=vector()
    cv.cor=vector()
    for (i in 1:length(kfoldmod$fits)/2) {
      fit2 = kfoldmod$fits[[i]]
      data = fit1$data %>% dplyr::select(-COTSExp08.12)
      nas = which(is.na(fit1$data$COTSExp85.12))
      nas = c(nas, which(is.na(fit1$data$predHCmaxmean)))
      preds <- posterior_predict(fit2, newdata = data[-nas,])
      pred <- colMeans(preds)
      cv.auc[i] = roc(fit1$y,pred,percent=TRUE,col="#1c61b6",  print.auc=TRUE)$auc[1]
      cv.cor[i] = cor(fit1$y, pred)
    }
    # browser()
    return(list(CV.AUC=mean(cv.auc), CV.COR=mean(cv.cor)))
}
   
mean(k.foldAUC(COTS_stanglm_PA, PA.kfold))

    # y = fit1$y
    pr <- as.integer(pred >= 0.5)
    # confusion matrix
    roc(fit1$y,pred,percent=TRUE,col="#1c61b6",  print.auc=TRUE)$auc[1]
nas = COTS_PA.sc[which(is.na(COTS_PA.sc)),]
AUC(COTS_stanglm_PAAC)
# linpred <- posterior_predict(fit1, newdata=na.omit(COTS_PA.sc), transform=TRUE)

formPA = as.formula(paste(c("COTS_PA~", paste(hypoth[-which(hypoth %in% c("CROSS_SHEL", "bent.clust.18","AC", "ACConn"))], collapse = "+"), "+ (1 |CROSS_SHELF)"), collapse = ""))
formPAAC = as.formula(paste(c("COTS_PA~", paste(hypoth[-which(hypoth %in% c("CROSS_SHEL","bent.clust.18","ACConn"))], collapse = "+"), "+ (1 |CROSS_SHELF)"), collapse = ""))
formPAACConn = as.formula(paste(c("COTS_PA~", paste(hypoth[-which(hypoth %in% c("CROSS_SHEL","bent.clust.18","AC"))], collapse = "+"), "+ (1 |CROSS_SHELF)"), collapse = ""))

formPROP = as.formula(paste(c("COTS_PROP~", paste(hypoth[-c(6,17,19,20)], collapse = "+"), "+ (1 |CROSS_SHELF)"), collapse = ""))
formPROP.AC = as.formula(paste(c("COTS_PROP~", paste(hypoth[-c(6,17,20)], collapse = "+"), "+ (1 |CROSS_SHELF)"), collapse = ""))
formPROP.ACConn = as.formula(paste(c("COTS_PROP~", paste(hypoth[-c(6,17,19)], collapse = "+"), "+ (1 |CROSS_SHELF)"), collapse = ""))
formOUT = as.formula(paste(c("COTS_OUT~", paste(hypoth[-c(6,17,19,20)], collapse = "+"), "+ (1 |CROSS_SHELF)"), collapse = ""))
formOUT.AC = as.formula(paste(c("COTS_OUT~", paste(hypoth[-c(6,17,20)], collapse = "+"), "+ (1 |CROSS_SHELF)"), collapse = ""))
formOUT.ACConn = as.formula(paste(c("COTS_OUT~", paste(hypoth[-c(6,17,19)], collapse = "+"), "+ (1 |CROSS_SHELF)"), collapse = ""))
### Presence Absence Models ----


COTS_stanglm_PA <- stan_glmer(formula = formPA,
                              data = COTS_PA.sc, family = binomial(link = "logit"), 
                              # prior = student_t(df = 7), 
                              # prior_intercept = student_t(df = 7),
                              chains = 3,
                              adapt_delta = 0.98,
                              iter = 10000,
                              thin = 5,
                              warmup=1000, seed = 123)
COTS_brm_PA <- brm(formula = formPA,
                              data = COTS_PA.sc, family = bernoulli(),
                              warmup = 1000, iter = 2000, chains = 4,
                              control = list(adapt_delta = 0.95))

summary(COTS_stanglm_PA)
COTS_stanglm_PAAC <- stan_glmer(formula = formPAAC,
                              data = COTS_PA.sc, family = binomial(link = "logit"), 
                              # prior = student_t(df = 7), 
                              # prior_intercept = student_t(df = 7),
                              chains = 3,
                              adapt_delta = 0.98,
                              iter = 10000,
                              thin = 5,
                              warmup=1000, seed = 123)
COTS_stanglm_PAACConn <- stan_glmer(formula = formPAACConn,
                              data = COTS_PA.sc, family = binomial(link = "logit"), 
                              # prior = student_t(df = 7), 
                              # prior_intercept = student_t(df = 7),
                              chains = 3,
                              adapt_delta = 0.98,
                              iter = 10000,
                              thin = 5,
                              warmup=1000, seed = 123)
AUC(COTS_stanglm_PA)
AUC(COTS_stanglm_PAAC)
AUC(COTS_stanglm_PAACConn)
mean(bayes_R2(COTS_stanglm_PA))
mean(bayes_R2(COTS_stanglm_PAAC))
mean(bayes_R2(COTS_stanglm_PAACConn))

# The Distance autocovar wins

k.foldAUC(COTS_stanglm_PAAC, PA.AC.kfold)



#(plotPA = ploteffects(COTS_stanglm_PAACConn))
PA.kfold = rstanarm::kfold(COTS_stanglm_PA, K = 10,save_fits = T)
PA.AC.kfold = rstanarm::kfold(COTS_stanglm_PAAC, K = 10,save_fits = T)
#PA.ACConn.kfold = rstanarm::kfold(COTS_stanglm_PAACConn, K = 10,save_fits = T)

plot(COTS_stanglm_PAAC, "areas", prob = 0.95, prob_outer = 1,pars = names(COTS_stanglm_PAAC$coefficients))

mean(bayes_R2(COTS_stanglm_PA))
mean(bayes_R2(COTS_stanglm_PAAC))
mean(bayes_R2(COTS_stanglm_PAACConn))
cor(PA.AC.kfold[["fits"]][[1]]$fitted.values,(PA.AC.kfold[["fits"]][[1]]$y))

#launch_shinystan(COTS_stanglm_PA)
### Persistence Models ----
COTS_stanglm_PROP <- stan_glmer(formula = formPROP,
                              data = COTS_PA.sc, family = poisson(link = "log"), 
                              # prior = student_t(df = 7), 
                              # prior_intercept = student_t(df = 7),
                              chains = 3,
                              adapt_delta = 0.98,
                              iter = 10000,
                              thin = 5,
                              warmup=1000, seed = 123)
COTS_stanglm_PROP.AC <- stan_glmer(formula = formPROP.AC,
                              data = COTS_PA.sc, family = poisson(link = "log"), 
                              # prior = student_t(df = 7), 
                              # prior_intercept = student_t(df = 7),
                              chains = 3,
                              iter = 10000,
                              adapt_delta = 0.98,
                              thin = 5,
                              warmup=1000, seed = 123)
COTS_stanglm_PROP.ACConn <- stan_glmer(formula = formPROP.ACConn,
                              data = COTS_PA.sc, family = poisson(link = "log"), 
                              # prior = student_t(df = 7), 
                              # prior_intercept = student_t(df = 7),
                              chains = 3,
                              iter = 10000,
                              adapt_delta = 0.98,
                              thin = 5,
                              warmup=1000, seed = 123)
PROP.kfold = rstanarm::kfold(COTS_stanglm_PROP, K = 10,save_fits = T)
PROP.AC.kfold = rstanarm::kfold(COTS_stanglm_PROP.AC, K = 10,save_fits = T)
PROP.ACConn.kfold = rstanarm::kfold(COTS_stanglm_PROP.ACConn, K = 10,save_fits = T)

### Outbreak models ----
COTS_stanglm_OUT <- stan_glmer(formula = formOUT,
                              data = COTS_Out.sc, family = binomial(link = "logit"), 
                              # prior = student_t(df = 7), 
                              # prior_intercept = student_t(df = 7),
                              chains = 3,
                              adapt_delta = 0.98,
                              iter = 10000,
                              thin = 5,
                              warmup=1000, seed = 123)
COTS_stanglm_OUT.AC <- stan_glmer(formula = formOUT.AC,
                              data = COTS_Out.sc, family = binomial(link = "logit"), 
                              # prior = student_t(df = 7), 
                              # prior_intercept = student_t(df = 7),
                              chains = 3,
                              adapt_delta = 0.98,
                              iter = 10000,
                              thin = 5,
                              warmup=1000, seed = 123)
COTS_stanglm_OUT.ACConn <- stan_glmer(formula = formOUT.ACConn,
                              data = COTS_Out.sc, family = binomial(link = "logit"), 
                              # prior = student_t(df = 7), 
                              # prior_intercept = student_t(df = 7),
                              chains = 3,
                              adapt_delta = 0.98,
                              iter = 10000,
                              thin = 5,
                              warmup=1000, seed = 123)
OUT.kfold = rstanarm::kfold(COTS_stanglm_OUT, K = 10,save_fits = T)
OUT.AC.kfold = rstanarm::kfold(COTS_stanglm_OUT.AC, K = 10,save_fits = T)
OUT.ACConn.kfold = rstanarm::kfold(COTS_stanglm_OUT.ACConn, K = 10,save_fits = T)

save.image(file = "BayesModels.RData")

(plotPA = ploteffects(COTS_stanglm_PA))
plotPROP = ploteffects(COTS_stanglm_PROP)
plotOUT = ploteffects(COTS_stanglm_OUT) 
gridExtra::grid.arrange(plotPA, plotPROP, plotOUT, ncol=1)
loo1 = rstanarm::loo(COTS_stanglm_PA, k_threshold = 0.7)
kfold1 = rstanarm::kfold(COTS_stanglm_PA, K = 10)
kfold1 = rstanarm::kfold(COTS_stanglm_PAAC, K = 10)
kfold1 = rstanarm::kfold(COTS_stanglm_PAACConn, K = 10)
launch_shinystan(COTS_stanglm_PA)


```
### Evaluation

```{r}
load("BayesModels.RData")
R2 = function(stanmod) {
  rowSums(posterior_linpred(stanmod)^2) / rowSums(posterior_predict(stanmod)^2)}
mean(R2(stanmod =COTS_stanglm_PAACConn))

# eval.stats.bayes = function(bayesmodels) {
#   stats = data.frame(R2=NA, LOOIC=NA, LOOIC.se=NA, elpd_kfold=NA, se_elpd_kfold=NA, AUC=NA, cv.AUC)
#   for(i in 1:length(bayesmodels)) {
#     #loo.mod = rstanarm::loo(bayesmodels[[i]], k_threshold=0.7)
#     k.fold.mod = rstanarm::kfold(bayesmodels[[i]], K = 10)
#     # stats[i,1] = mean(rowSums(posterior_linpred(bayesmodels[[i]])^2) / rowSums(posterior_predict(bayesmodels[[i]])^2))
#     #stats[i,2] = loo.mod$looic 
#     #stats[i,3] = loo.mod$se_looic
#     stats[i,4] = k.fold.mod$elpd_kfold
#     stats[i,5] = k.fold.mod$se_elpd_kfold
#     stats[i,6] = AUC(bayesmodels[[i]])
#     stats[i,7] = mean(k.foldAUC(bayesmodels[[i]], k.fold.mod))
#     }
#   return(list(stats, loo.mod, k.fold.mod))
# }

eval.stats.bayes = function(bayesmodels, k.foldmods) {
  stats = data.frame(R2=NA, LOOIC=NA, LOOIC.se=NA, elpd_kfold=NA, se_elpd_kfold=NA, AUC=NA, cv.AUC)
  for(i in 1:length(bayesmodels)) {
    stats[i,1] = mean(rowSums(posterior_linpred(bayesmodels[[i]])^2) / rowSums(posterior_predict(bayesmodels[[i]])^2))
    stats[i,4] = k.fold.mod$elpd_kfold
    stats[i,5] = k.fold.mod$se_elpd_kfold
    stats[i,6] = AUC(bayesmodels[[i]])
    stats[i,7] = mean(k.foldAUC(bayesmodels[[i]], k.fold.mods[[i]]))
    }
  return(stats)
}

evaluation.stan  = function(stanmodels, kfoldmods) {
  stats = data.frame(model = names(stanmodels), dev.expl=NA, Training.AUC=NA, CV.AUC=NA, cv.cor=NA, weighted=NA )
  for(i in 1:length(stanmodels)){
    kfoldstats = k.foldAUC(stanmodels[[i]], kfoldmods[[i]])
    stats[i,2] = mean(bayes_R2(stanmodels[[i]]))
    stats[i,3] = AUC(stanmodels[[i]])
    stats[i,4] = kfoldstats$CV.AUC
    stats[i,5] = kfoldstats$CV.COR
    stats[i,6] = (stats[i,2] + stats[i,5])/2
    }
  stats = stats %>% arrange(-weighted)
  return(stats)
}







# Xmat = model.matrix(formPA, data=model.frame(formPA,data=COTS_PA.sc))
# coefs = as.matrix(loyn.rstanarm)
# (i=which(colnames(coefs)=='sigma'))
# coefs=coefs[,-i]
# 
# fit = coefs %*% t(Xmat)
# resid = sweep(fit, 2, loyn$ABUND, "-")
# var_f = apply(fit, 1, var)
# var_e = apply(resid, 1, var)
# R2 = var_f/(var_f + var_e)
# tidyMCMC(as.mcmc(R2), conf.int = TRUE, conf.method = "HPDinterval")


```

## Ensenmble prediction

```{r Ensemble Prediction}
PA_brtPRED = gbm::predict.gbm(CoTS.gbmPA.AC, XYZsum.Conn.sc, n.trees=CoTS.gbmPA.AC$gbm.call$best.trees, type="response")
PROP_brtPRED = gbm::predict.gbm(CoTS.gbmPROP.ACConn, XYZsum.Conn.sc, n.trees=CoTS.gbmPROP.ACConn$gbm.call$best.trees, type="response")
OUT_brtPRED = gbm::predict.gbm(CoTS.gbmOUT.ACConn, XYZsum.Conn.sc, n.trees=CoTS.gbmOUT.ACConn$gbm.call$best.trees, type="response")

ensemble = rbind(evaluation(list(gbmPA=CoTS.gbmPA.AC)),
                 evaluation.stan(stanmodels = list(stanPA=COTS_stanglm_PAAC), kfoldmods = list(PA.AC.kfold)))



#### GBM Predict ----

newdata = XYZsum.Conn.sc %>% dplyr::select(-COTSExp08.12, -c(4:7)) %>% filter(!is.na(COTSExp85.12)) %>% filter(!is.na(predHCmaxmean)) 

preds.gbm.PAAC = gbm::predict.gbm(CoTS.gbmPA.AC, newdata, n.trees=CoTS.gbmPA.AC$gbm.call$best.trees, type="response")
preds.gbm.PA = gbm::predict.gbm(CoTS.gbmPA, newdata, n.trees=CoTS.gbmPA$gbm.call$best.trees, type="response")

#### STAN Predict ----

preds.STAN.PAAC = colMeans(posterior_predict(COTS_stanglm_PAAC, newdata = newdata))
preds.STAN.PA = colMeans(posterior_predict(COTS_stanglm_PA, newdata = newdata))
preds.df = cbind(newdata, GBMPred.AC = preds.gbm.PAAC, STANPred.AC = preds.STAN.PAAC, 
                 GBMPred = preds.gbm.PA, STANPred = preds.STAN.PA) %>%
           mutate(Ensemble.meanAC = rowMeans(select(.,GBMPred.AC, STANPred.AC)),
                   Ensemble.mean = rowMeans(select(.,GBMPred, STANPred)),
                   Ensemble.WAAC = (ensemble$weighted[1]*preds.gbm.PAAC + ensemble$weighted[2]*preds.STAN.PAAC)/
                                 (ensemble$weighted[1]+ensemble$weighted[2]))






#### inspect using POWER BI  
write.csv(preds.df %>% left_join(dplyr::select(XYZsum.Conn.sc, REEF_NAME, COTS_PA), by="REEF_NAME"), "preds.PA.csv", row.names = F)
write.csv(newdata, "XYZsum.csv", row.names = F)

preds.df = preds.df %>% left_join(dplyr::select(XYZsum.Conn.sc, REEF_NAME, COTS_PA))
ggplot(preds.df, aes(x=Ensemble.meanAC, y=COTS_PA)) + 
  geom_point() + 
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = TRUE)
```

```{r Plot Predictions}
# FIG 3 ----------

library(PBSmapping)
library(RColorBrewer)
library(colorRamps)
# Import and rotate spatial layers
shape <- importShapefile("Data/SDE_OWNER_crcgis_land250/SDE_OWNER_crcgis_land250", readDBF=FALSE)

shape.poly <- rgdal::readOGR("Data/SDE_OWNER_crcgis_land250/SDE_OWNER_crcgis_land250.shp", p4s = "+proj=longlat +datum=WGS84")
shape.elide <- elide(shape.poly, rotate=45)
shape45 <- SpatialPolygons2PolySet(shape.elide)

reefs.poly <- rgdal::readOGR("Data/Indicative_Reef_boundary/Indicative_Reef_boundary.shp", p4s = "+proj=longlat +datum=WGS84")
reefs.elide <- elide(reefs.poly, rotate=45)
reefs45 <- SpatialPolygons2PolySet(reefs.elide)

# grid <- SpatialPoints(cbind(data.grid$LONG, data.grid$LAT), proj4string = CRS("+proj=longlat +datum=WGS84"))
# grid.elide <- elide(grid, bb=bbox(shape.poly), rotate=45)
# grid45 <- as.data.frame(grid.elide)

# pst.grid <- SpatialPoints(cbind(data.WQ$X, data.WQ$Y), proj4string = CRS("+proj=longlat +datum=WGS84"))
# pst.elide45 <- elide(pst.grid, bb=bbox(shape.poly), rotate=45)
# pst45 <- as.data.frame(pst.elide45)

gridln <- gridlines(shape.poly)
gridln.elide <- elide(gridln, bb=bbox(shape.poly), rotate=45)
gridln45 <- SpatialLines2PolySet(gridln.elide)

# extrapol <- importShapefile("/Users/Camille/Dropbox/My documents/Projects/CoTS/Modelling/Coral Cover Model_update/shapefiles/Extrapolated_buffer_20km", readDBF=FALSE)
# extrapol.poly <- readShapePoly("/Users/Camille/Dropbox/My documents/Projects/CoTS/Modelling/Coral Cover Model_update/shapefiles/Extrapolated_buffer_20km", proj4string = CRS("+proj=longlat +datum=WGS84"))
# extrapol.elide <- elide(extrapol.poly, bb=bbox(shape.poly), rotate=45)
# extrapol45 <- SpatialPolygons2PolySet(extrapol.elide)


# Plot map (left panel)
dev.off()
# par(mai=c(.5,.6,.3,.3), omi=c(0,0,0,0), family="Calibri", cex=1, cex.lab=1.5, cex.sub=1.5, cex.axis=1.5)
# layout.show(layout(rbind(c(1,2),c(1,3),c(1,4)), widths= c(1,1), heights = c(1,1,1)))

color=matlab.like(length(preds.df$Ensemble.mean))[rank(preds.df$Ensemble.mean)]
par(mai=c(0,0,0,0))
plotPolys(shape45, col="gray95", border="gray70", xlim=c(151,156), ylim=c(-36, -22), bg="white", cex=1.2, axes=F, ylab=NA, plt=c(0,1,0,1), colHoles=NA)
addPolys(reefs45, col=color, border="gray70", bg="white")
# points(pst45$x, pst45$y, pch=19, cex=1, col=rgb(greys(scale(data.WQ$PST)),max=255))
lines(gridln.elide, col="lightgrey")
addPolys(shape45, col="gray95", border="gray70", bg="white", colHoles=NA)
points(grid45$x, grid45$y, pch=19, cex=1, col=decdis.kol)
addPolys(extrapol45, col=addalpha("white",.4), border="white", lwd=2, colHoles=NA)
addPolys(extrapol45, col="lightgrey", border="lightgrey", lwd=2, density=8, colHoles=NA)

northarrow(loc=c(152,-24), size=.6, cex=2, bearing=-pi/4)
text(155.7,-22.5,"A)", cex=2, font=2)

text(155.3, -22.5, "150E", col="lightgrey", srt=40, cex=1.5)
text(155.3, -29.4, "155E", col="lightgrey", srt=40, cex=1.5)
text(155.3, -26, "15S", col="lightgrey", srt=-40, cex=1.5)
text(155.3, -33.2, "20S", col="lightgrey", srt=-40, cex=1.5)

#Legend
x0 <- 155.7
y0 <- -35.8

arrows(x0-1.2, y0+1.7, x0+.1, y0+.4, length=.1, lwd=2)

rect(x0-1.1, y0+.5, x0-.6, y0+1, col="darkseagreen1")
rect(x0-1.1, y0+1.1, x0-.6, y0+1.6, col="steelblue3")
rect(x0-.5, y0+.5, x0, y0+1, col="mediumseagreen")
rect(x0-.5, y0+1.1, x0, y0+1.6, col="lightskyblue2")

text(x0-.85, y0+.3, "L", cex=1.5)
text(x0-.25, y0+.3, "H", cex=1.5)
text(x0-1.3, y0+.75, "L", cex=1.5)
text(x0-1.3, y0+1.35, "H", cex=1.5)
text(x0-.55, y0, "Disturbance", cex=1.5, font=2)
text(x0-1.5, y0+.95, "Decline",srt=90, cex=1.5, font=2)
text(x0+.2, y0+.3, "R", cex=2, font=4)


# WQ plot
gam <- gam(resilience.sub ~ s(WQ.sub, k=3, bs="cs"))
new.data <- data.frame(WQ.sub=seq(0,1,by=0.01))
pred <- predict.gam(gam, new.data, se.fit=T)
pred.mn <- pred$fit
pred.hi <- pred$fit + 1.96*pred$se.fit
pred.lo <- pred$fit - 1.96*pred$se.fit

par(mai=c(.6,.6,.1,.1))
plot(resilience.sub ~ WQ.sub, type="n", xlab="PFc", ylab="Resilience (R)", xaxs="i", yaxs="i", xlim=c(0,1), ylim=c(-2,3))
polygon(c(new.data[,1], rev(new.data[,1])), c(pred.hi, rev(pred.lo)), col="lightgrey", border=NA)
points(WQ.sub, resilience.sub, pch=16, col=addalpha("seashell2",.5))
lines(new.data$WQ.sub, pred$fit, col="darkgrey", lwd=2)
rug(WQ.sub)
text(.9,2.5, "B)", cex=2, font=2)
box()

# Gravity plot
gam <- gam(resilience.sub ~ s(gravity.log, k=3, bs="cs"))
new.data <- data.frame(gravity.log=seq(0,0.6, by=0.01))
pred <- predict.gam(gam, new.data, se.fit=T)
pred.mn <- pred$fit
pred.hi <- pred$fit + 1.96*pred$se.fit
pred.lo <- pred$fit - 1.96*pred$se.fit

par(mai=c(.6,.6,.1,.1))
plot(resilience.sub ~ gravity.log, type="n", xlab="Human density", ylab="Resilience (R)", xaxs="i", yaxs="i", xlim=c(0,0.5), ylim=c(-2,3))
polygon(c(new.data[,1], rev(new.data[,1])), c(pred.hi, rev(pred.lo)), col="lightgrey", border=NA)
points(gravity.log, resilience.sub, pch=16, col=addalpha("seashell2",.5))
lines(new.data$gravity.log, pred$fit, col="darkgrey", lwd=2)
rug(gravity.log)
text(.45,2.5, "C)", cex=2, font=2)
box()


# Green zone plot
par(mai=c(.6,.6,.1,.1))
vioplot(resilience.sub[green.sub==0], resilience.sub[green.sub==1], 
        resilience2[green.sub2==0 & WQ.sub2<.05], resilience2[green.sub2==1 & WQ.sub2<.05],
        col=c("seashell1","seashell3","seashell1","seashell3"), names=c("Open","Closed","Open","Closed"),
        ylim = c(-2,4), at=c(1,2,3.5,4.5))
title(ylab="Resilience (R)", xlab="Reef zoning")
text(4.8,3.7, "D)", cex=2, font=2)
text(1.5,3.7, "All reefs", cex=1.5)
text(4,3.7, "PFc < 0.5", cex=1.5)





```




```